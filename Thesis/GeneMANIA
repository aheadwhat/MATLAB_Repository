function [f, mu] = GeneMANIA(W,y,beta, gamma, rescale, regularize)
%% GeneMANIA refined script
%--------
% INPUT: |
%--------
% W: cell array
%    stores the similarity matrices
% y: nx1 vetor
%    label vector(-1, +1, 0)
% beta: positive scalar
%    control the smoothness fitness
% gamma: positive scalar
%    control the penalty strength for the regularized version
% rescale: logic
%    indicate whether to use the normalization operation mentioned in the
%    paper
% regularize: logic
%    decides whether to use the regularization version or not
%--------
% INPUT: |
%--------
% f: nx1 vector
%    prediction vector
% mu: Px1 vector
%    weight vector for each graph
%%
if nargin < 2
    error('cell array and label vector must be provided!');
end
if nargin < 3
    beta = 0.1;
end
if nargin < 4
    gamma = 0.1;
end
if nargin < 5
    rescale = true;
end
if nargin < 6
    regularize = true;
    disp('default: use the regularization version');
end

if rescale
    disp('normalize GeneMANIA');
    W = normalizeKernel(W);
end

P = size(W, 2);  % #(graphs/network)
n = size(W{1},1); % #(samples)
mu = zeros(P,1); 

if regularize
    [w,idx] = findKernelWeightsReg(y, W, gamma);
else
    [w,idx] = findKernelWeights(y, W);
    
end



mu(idx) = w;
wW = zeros(n);
for i = 1:length(idx)
    wW = wW + W{idx(i)}  * w(i);
    
end

NN = nnz(y == 1);
MM = nnz(y == -1);
% seems curious!!!
y(y==0) = (NN-MM) / (NN + MM); % set the label bias to the mean of the labels
D = diag(1./sqrt(max(sum(wW,2),0.001)));
wL = eye(n) - D*wW*D;
f = beta.*((beta * eye(n) + wL)\y);

% %% original version to solve the LP by CG
% %% which I think is not necessary....
% [f,~] = getScoreVectorCG(y, wL);



end


function [alpha,indices, bias] = findKernelWeightsReg(y, W, gamma)% gamma added by me...
%findKernelWeights -- find ridge regression estimates for kernel weights
%
%  [ALPHA, INDICES] = findKernelWeightsReg(y, W)
%
%   y -- vector of labels  y(i) is -1, 0, or 1
%   W -- cell array of sparse matrices.  W{I} is the I-th affinity network
%        aka weight matrix aka kernel.  W{I} is assumed to be
%        sparse and symmetric.
%
%  If calculations yield negative weights for any kernels, those kernels
%  are removed and alpha is recomputed with the remaining kernels. This
%  process is repeated until only positive weights remain.
%
%  This function eliminates terms which correspond to negative/negative
%  pairs in the target matrix before performing the linear regression.
%  Thus far it appears that this modification does not result in significantly
%  better results, but may speed up computation significantly in some
%  situations.
%
%  Returns ALPHA, a vector of weights for matrices (not including those
%  which received 0 weight), and INDICES, a vector containing the indices
%  of the kernels which had non-zero weights, in the same order as the
%  elements of ALPHA.
%
%  If and ONLY IF a third output argument is specified, the regression is
%  done with a bias term.
%  $ Revision 1.2 $
%  $ Revision 1.3 $ Sara -- the weights are regularized to be close to
%  equall weights (ie. a Gaussian weight prior with mean being equal for
%  all weights). The precision matrix is a diagonal matrix with elements
%  equal to Trace(WtW). ------>> modify the matrix KtKp and alphap to set
%  the regularization prior and strength

% ---- Setting a few constants
% usesBias = 1;
ixPos = find(y == 1);
ixNeg = find(y == -1);
nPos = nnz(y == 1);
nNeg = nnz(y == -1);
NN = length(y);
MM = length(W);
usesBias = (nargout > 2);  % flag for the use of a bias term
epsilon = eps;             % our numerical def'n of zero

posConst = 2*nNeg / (nPos + nNeg);   % Label bias for +/+ elements
negConst = -2*nPos / (nPos + nNeg);  % Label bias for +/- elements

% ---- A few checks on the arguments.

if nPos == 0 || nNeg == 0
    error('Must have at least one positive and one negative');
end

nZero = nnz(y == 0);
if nPos + nNeg + nZero ~= length(y)
    error('Elements of Y must be either -1, 0, or 1');
end

if NN == 0, error('Empty target vector!'); end
if MM == 0, error('Empty kernel set'); end

% ---- Calculation of the linear indices in W{K} of all the
%      (i,j)-elements for which
%  ixpp:  y(i) = 1 and y(j) = 1 and i ~= j,   (aka +/+)
%  ixpn:  y(i) = 1 and y(j) = -1              (aka +/-)

% Note: because the affinity matrices in W are symmetric, all the sums
% and dot products involving +/- (i,j) elements are the same for the
% -/+ (j,i) elements.  So, we simply calcuate double any sums and dot
% products that we calculate over the +/- elements.

% ---- Calculation of the two matrices needed to do the linear regression:
%  KtK:  the gram matrix containing the dot products of the kernels
%  KtT:  the dot products of the kernels with the target values

KtK = zeros(MM+1, MM+1);
KtT = zeros(MM+1, 1);


% ---- calculating the prior mean and precision matrix
% KtKp: a diagonal matrix where KtKp(ii,ii) = sum(sum(W{ii}.^2))
% alphap: a vector of length d, where d is the number of kernels,
% alphap(ii) = 1/d

KtKp = zeros(MM+1, MM+1);
alphap = zeros(MM+1, 1);

% set the prior bias and KtKp to zero so not to penalize them
KtKp(1,1) = 0;
alphap(1,1) = 0;

Wpp = cell(1, MM);    % temp storage of +/+ non-diagonal affinities
Wpn = cell(1, MM);    % temp storage of +/- elements affinities
nPpElem = nPos * (nPos - 1); % # +/+ elements that aren't diagonal
nPnElem = 2 * nPos * nNeg;   % # +/- elements
ppTarget = posConst^2;           % target for +/+ elements
pnTarget = posConst * negConst;  % target for +/- elements
elem = (nPpElem + nPnElem);
% value of the bias interactions
biasVal = 1 / (nPpElem + nPnElem);

KtT(1) = biasVal * (ppTarget * nPpElem + pnTarget * nPnElem);
KtK(1,1) = biasVal;


for ii = 1:MM
    Wpp{ii} = W{ii}(ixPos, ixPos);
    tmp = sparse(1:nPos, 1:nPos, diag(Wpp{ii}));
    Wpp{ii} = Wpp{ii} - tmp;   % sets the diagonal to zero
    Wpn{ii} = W{ii}(ixPos, ixNeg);
    
    if(~usesBias)
        md = (sum(sum(Wpp{ii})) + 2 * sum(sum(Wpn{ii}))) / elem;
        Wpn{ii} = Wpn{ii} - md;
        Wpp{ii} = Wpp{ii} - md;
    end
    
    ssWpp = full(sum(sum(Wpp{ii})));
    ssWpn = full(sum(sum(Wpn{ii})));
    
    KtT(ii+1) = ppTarget * ssWpp + 2 * pnTarget * ssWpn;
    KtK(ii+1, 1) = biasVal * (ssWpp + 2 * ssWpn);
    KtK(1, ii+1) = KtK(ii+1, 1);
    
    for jj = 1:ii
        KtK(ii+1,jj+1) = full(...
            sum(sum(Wpp{ii} .* Wpp{jj})) + ...
            2 * sum(sum(Wpn{ii} .* Wpn{jj})) ...
            );
        KtK(jj+1,ii+1) = KtK(ii+1,jj+1);   % make KtK symmetric
    end
end

const = nPos + nNeg;
% ########################################################################
% ---------------------------------Crucial PART?--------------------------
% ---- calculating KtKp
for ii = 1:MM
    [rr,cc,ss] = find(W{ii});
%     KtKp(ii+1,ii+1) = sum(ss.^2)/sqrt(length(ss)); % --->> adjust this to control the amount  of regularization
    KtKp(ii+1,ii+1) = gamma; % my version: change penalty term?
end

% ---- setting the prior mean

% alphap(2:MM) = 1; % original version
alphap(2:MM) = 0; % my version:maybe degenerated to standard L2 version?

%#########################################################################
% ---------------------------------Crucial PART END--------------------------
%

% ---- Removing empty columns

ss = sum(abs(KtK+KtKp), 2);
goodIdx = find(sum(abs(KtK+KtKp), 2) > epsilon * max(ss));
KtK = KtK(goodIdx, goodIdx);
KtT = KtT(goodIdx);
KtKp = KtKp(goodIdx,goodIdx);
alphap = alphap(goodIdx);

indices = 1:(length(goodIdx) - 1);


done = 0;

% we are finding alpha that minimizes: (alpha^t*K - T)^2 - (alpha -
% alphap)KtKp(alpha - alphap)

while ~done
    
    if isempty(indices)
        break
    end
    
    
    % alpha = (K' * K)^-1 K' * T(:)
    % alpha = (inv(KtKp+KtK)*(KtKp*alphap+KtT));
    
    alpha = (KtK + KtKp) \ (KtT + KtKp*alphap);
    
    if(any(isnan(alpha)))
        indices = [];
        break
    end
    
    % Find the locations of all the negative weights
    negWeights = find(alpha <= 0);
    
    negWeights = negWeights(:);
    negWeights = setdiff(negWeights, 1);
    if ~isempty(negWeights)
        
        % Save ourselves some computation by just eliminating rows and cols
        % from K^T K and elements from K^T y. The savings from this appear
        % to be nominal, though.
        KtK(:,negWeights) = [];
        KtK(negWeights,:) = [];
        KtT(negWeights) = [];
        alphap(negWeights) = [];
        KtKp(negWeights,:) = [];
        KtKp(:,negWeights) = [];
        % Remove from the indices array.
        indices((negWeights - 1)) = [];
    else
        done = 1;
        fprintf('%d matrices chosen, indices %s, weights %s\n',...
            size(KtK,1),mat2str(indices),mat2str(alpha));
    end
end

% Undo our previous trickery
indices = goodIdx(indices);



% if we have no non-zero alpha entries, use the average kernel
if isempty(indices)
    indices = 1:length(W);
    alpha = [0 1/length(W)*ones(1,length(W))]';
    fprintf('All kernels eliminated or empty, using average kernel\n');
end

bias = alpha(1);
alpha = alpha(2:end);


end



function [alpha,indices] = findKernelWeights(y, W)
%findKernelWeights -- find least squares estimates for kernel weights
%
%  [ALPHA, INDICES] = findKernelWeights(y, W)
%
%   y -- vector of labels  y(i) is -1, 0, or 1
%   W -- cell array of sparse matrices.  W{I} is the I-th affinity network
%        aka weight matrix aka kernel.  W{I} is assumed to be
%        sparse and symmetric.
%   Other Notes: similar to the regularized version


% ---- Setting a few constants
ixPos = find(y == 1);
ixNeg = find(y == -1);
nPos = nnz(y == 1);
nNeg = nnz(y == -1);
NN = length(y);
MM = length(W);
usesBias = 1 ; % always use bias
epsilon = eps;             % our numerical def'n of zero

posConst = 2*nNeg / (nPos + nNeg);   % Label bias for +/+ elements
negConst = -2*nPos / (nPos + nNeg);  % Label bias for +/- elements

% ---- A few checks on the arguments.

if nPos == 0 || nNeg == 0
    error('Must have at least one positive and one negative');
end

nZero = nnz(y == 0);
if nPos + nNeg + nZero ~= length(y)
    error('Elements of Y must be either -1, 0, or 1');
end

if NN == 0, error('Empty target vector!'); end
if MM == 0, error('Empty kernel set'); end

% ---- Calculation of the linear indices in W{K} of all the
%      (i,j)-elements for which
%  ixpp:  y(i) = 1 and y(j) = 1 and i ~= j,   (aka +/+)
%  ixpn:  y(i) = 1 and y(j) = -1              (aka +/-)

% Note: because the affinity matrices in W are symmetric, all the sums
% and dot products involving +/- (i,j) elements are the same for the
% -/+ (j,i) elements.  So, we simply calcuate double any sums and dot
% products that we calculate over the +/- elements.

% ---- Calculation of the two matrices needed to do the linear regression:
%  KtK:  the gram matrix containing the dot products of the kernels
%  KtT:  the dot products of the kernels with the target values

KtK = zeros(MM+1, MM+1);
KtT = zeros(MM+1, 1);

Wpp = cell(1, MM);    % temp storage of +/+ non-diagonal affinities
Wpn = cell(1, MM);    % temp storage of +/- elements affinities
nPpElem = nPos * (nPos - 1); % # +/+ elements that aren't diagonal
nPnElem = 2 * nPos * nNeg;   % # +/- elements
ppTarget = posConst^2;           % target for +/+ elements
pnTarget = posConst * negConst;  % target for +/- elements
elem = (nPpElem + nPnElem);
% value of the bias interactions
biasVal = 1 / (nPpElem + nPnElem);

KtT(1) = biasVal * (ppTarget * nPpElem + pnTarget * nPnElem);
KtK(1,1) = biasVal;

for ii = 1:MM
    Wpp{ii} = W{ii}(ixPos, ixPos);
    tmp = sparse(1:nPos, 1:nPos, diag(Wpp{ii}));
    Wpp{ii} = Wpp{ii} - tmp;   % sets the diagonal to zero
    Wpn{ii} = W{ii}(ixPos, ixNeg);
    
    ssWpp = full(sum(sum(Wpp{ii})));
    ssWpn = full(sum(sum(Wpn{ii})));
    
    KtT(ii+1) = ppTarget * ssWpp + 2 * pnTarget * ssWpn;
    KtK(ii+1, 1) = biasVal * (ssWpp + 2 * ssWpn);
    KtK(1, ii+1) = KtK(ii+1, 1);
    
    for jj = 1:ii
        KtK(ii+1,jj+1) = full(...
            sum(sum(Wpp{ii} .* Wpp{jj})) + ...
            2 * sum(sum(Wpn{ii} .* Wpn{jj})) ...
            );
        KtK(jj+1,ii+1) = KtK(ii+1,jj+1);   % make KtK symmetric
    end
end

% ---- Removes the bias term if its not used

if ~usesBias
    KtT = KtT(2:end);
    KtK = KtK(2:end, 2:end);
end

% ---- Removing empty columns

ss = sum(abs(KtK), 2);
goodIdx = find(sum(abs(KtK), 2) > epsilon * max(ss));
KtK = KtK(goodIdx, goodIdx);
KtT = KtT(goodIdx);

indices = 1:(length(goodIdx) - 1);

done = 0;


while ~done
    
    % alpha = (K' * K)^-1 K' * T(:)
%     alpha = KtK \ KtT; % orginal version, cannot pass my test...,
%     ill-conditioned
    alpha = pinv(KtK) * KtT; % my revision, use the pseudo inverse

    if(any(isnan(alpha)))
        indices = [];
        break
    end
    
    % Find the locations of all the negative weights
    negWeights = find(alpha < 0);
    
    negWeights = negWeights(:);
    negWeights = setdiff(negWeights, 1);  % always have a bias term now
    if length(negWeights) > 0
        
        % Save ourselves some computation by just eliminating rows and cols
        % from K^T K and elements from K^T y. The savings from this appear
        % to be nominal, though.
        KtK(:,negWeights) = [];
        KtK(negWeights,:) = [];
        KtT(negWeights) = [];
        
        % Remove from the indices array.
        indices((negWeights - 1)) = [];
    else
        done = 1;
        fprintf('%d matrices chosen, indices %s, weights %s\n',...
            size(KtK,1)-1,mat2str(indices),mat2str(alpha(2:end)));
    end
end




% Undo our previous trickery
indices = goodIdx(indices);


% if we have no non-zero alpha entries, use the average kernel
if length(indices) == 0
    indices = 1:length(W);
    alpha = [0 1/length(W)*ones(1,length(W))]';
    fprintf('All kernels eliminated or empty, using average kernel\n');
end
% ignore the bias term which is alpha(1)
alpha = alpha(2:end);


end



function WW = normalizeKernel(WW)
% function WWW = normalizeKernel(WW)

P = size(WW, 2);
for i = 1:P
    % Subtract any diagonal scores
    WW{i} = WW{i} - diag(diag(WW{i}));
    
    % Get the indices and values of all the non-zero elements
    [rr, cc, ss] = find(WW{i});
    % Get the column sums, which are the same as the row sums since the matrix is symmetrical
    DD = full(sum(WW{i}, 1));
    % Create a filehandle to represent the normalization equation
    norm = @(x) (1./sqrt(x));
    % Only normalize for the rows/columns where there is at least one non-zero interaction
    normalize = spfun(norm, DD);
    % Normalize by dividing the value by the product of the square-root of the column sum and the square-root of the row sum
    WW{i} = sparse(rr, cc, ss' .* normalize(rr) .* normalize(cc), size(WW{i}, 1), size(WW{i},2));
end
 
end


function [minX,res,ii] = conjGrad(initialX,b,A,maxIt)
% function minX = conjGrad(initialX,b,A)
% conjugate gradient method to find the minimum of vector X in the system
% b = A*x where A is the nxn Data matrix (kernel matrix here) and b is
% class label vector of nx1
% maxIt is the maximum iteration number


% required parameters:
r = b - A*initialX;
d = r;
x = initialX;

for ii = 1:maxIt
    
    step = (r'*r)/((d'*A)*d);
    x = x + step*d;
    rn = r - step*(A*d);
    
    Beta = (rn'*rn)/(r'*r);
    dn = rn+Beta*d;
    
    d = dn;
    r = rn;
    if(sum(r.^2) < 1e-16)
        break
    end
    
end

res = r;
minX = x;

end


function [f,numIt] = getScoreVectorCG(Y, Wk)
%getScoreVector -- Compute score vector given a kernel and a label vector
%                  using the conjugate gradient method
%
%   F = getScoreVector(Y, Wk)
%
%   Wk -- an affinity network represented as a kernel
%   Y  -- label vector: -1/+1 for training points, 0 for other points
%
% Returns F, a vector of discriminant values ("scores") derived by solving
% the system Z = Wk * F. where Z(I) = Y(I) if Y(I) = -1/+1, otherwise
%  Z(I) = (N-M)/(N+M) where N = nnz(Y = 1) and M = nnz(Y = -1), numIt is
%  the number of conjugate gradient iteration until convergence
%
% $Revision: 1.4 $

% Compute D^(-1/2), applying a tiny bit of regularization to make sure
% we're invertible.

% Compute D^(-1/2) L D^(-1/2), L = D - Wk, turns into this with some algebra.


Y = Y(:);
NN = nnz(Y == 1);
MM = nnz(Y == -1);
Y(Y==0) = (NN-MM) / (NN + MM); % set the label bias to the mean of the labels

oneI = sparse(1:length(Wk), 1:length(Wk), 1);

d = 1 ./ sqrt(sum(Wk,1) + eps);
[rr,cc,ss] = find(Wk);
Shat = ss(:)' .* d(rr) .* d(cc);
lD = length(d);
What =  sparse(rr,cc,Shat,lD,lD);
Dhat = sparse(1:lD, 1:lD, sum(What, 1));
Lhat = Dhat - What;
M = oneI + Lhat;
M = max(M,M'); % make sure M is symmetric

finit = zeros(size(Y,1),1);
maxit = 1000;  % hard-coded maximum number of iterations to 1000: usually it takes less than 25 iterations
[f,res,numIt] = conjGrad(finit,Y,M,maxit);

end
